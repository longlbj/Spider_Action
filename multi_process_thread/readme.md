1、解决重复抓取的问题：
1）将访问过的URL保存到数据库，并且url设置UNIQUE。（磁盘读写效率太低）
2）用HashSet将访问过的URL保存起来。那么只需接近o(1)的代价就可以查到一个URL是否被访问过。（放进内存读写快，但是有些url很长消耗内存）
3）URL经过MD5(哈希函数)等单向hash后再保存到HashSet或数据库。（哈希表碰撞）
4）Bit_Map方法。建立一个BitSet，将每个URL经过一个哈希函数映射到某一位。（md5哈希的基础上将128位进一步压缩哈希到一个bit位）
方法1~3都是将访问过的URL完整保存，方法4则只标记URL的一个映射位
**5）Bloom Filter：通过K个哈希函数映射到K位上，这样只有当新的URL计算得到的K位都为1时才判断为该URL已经访问过。


python实现一个多线程爬虫步骤：
1. 创建一个线程池 threads = []
2. 确认 url 队列线程安全 Queue Deque
3. 从队列取出 url,分配一个线程开始爬取 pop()/get() threading.Thread
4. 如果线程池满了,循环等待,直到有线程结束 t.is_alive()
5. 从线程池移除已经完成下载的线程 threads.remove(t)
6. 如果当前级别的url已经遍历完成,t.join() 函数等待所有现场结束,然后开
始下一级别的爬取

多线程爬虫评价：
优势:
• 有效利用CPU时间
• 极大减小下载出错、阻塞对抓取速度的影响,整体上提高下载的速
• 对于没有反爬虫限制的网站,下载速度可以多倍增加
局限性:
• 对于有反爬的网站,速度提升有限
• 提高了复杂度,对编码要求更高
• 线程越多,每个线程获得的时间就越少,同时线程切换更频繁也带来额外开销
• 线程之间资源竞争更激烈 

创建多进程爬虫：
我们采用数据库模式，就是把爬虫队列转移到数据库中，单独存储队列。这样多个爬取进程，URL的获取与增加都通过数据库操作。
优势：开发便捷，天生具备读写保护及支持IPC(进程间通信),只需要写一个爬虫程序。



